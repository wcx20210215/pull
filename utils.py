"""
utils - æ•°æ®åˆ†ææ™ºèƒ½ä½“ä½¿ç”¨çš„å·¥å…·å‡½æ•°

Author: éª†æ˜Š
Version: 0.1
Date: 2025/6/25
"""
import json
import streamlit as st
import time
from typing import List, Dict, Any, Generator

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain.schema import HumanMessage, AIMessage

PROMPT_TEMPLATE = """æ•°æ®åˆ†æåŠ©æ‰‹ç™»åœºï¼ğŸš€æ•°æ®åˆ†æå°±åƒä¸€åœºå†’é™©ï¼Œè€Œæˆ‘å°±æ˜¯ä½ çš„å‘å¯¼ã€‚âœ¨ä¸‹é¢æ˜¯æˆ‘çš„é­”æ³•æŒ‡å—ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¢ç´¢æ•°æ®çš„å¥¥ç§˜ï¼š

1. **æ€è€ƒé˜¶æ®µ (Thought)** ï¼šé¦–å…ˆï¼Œæˆ‘ä¼šåƒä¾¦æ¢ä¸€æ ·åˆ†æä½ çš„è¯·æ±‚ç±»å‹ï¼ˆæ–‡å­—å›ç­”/è¡¨æ ¼/å›¾è¡¨ï¼‰ï¼Œå¹¶éªŒè¯æ•°æ®æ˜¯å¦åˆé€‚ã€‚ğŸ”

2. **è¡ŒåŠ¨é˜¶æ®µ (Action)** ï¼šæ ¹æ®åˆ†æç»“æœï¼Œæˆ‘ä¼šç”¨ä»¥ä¸‹é­”æ³•æ ¼å¼æ¥å›åº”ä½ ï¼šâœ¨
   - **çº¯æ–‡å­—å›ç­”**ï¼šï¼ˆç®€æ´æ˜äº†ï¼Œç»ä¸å•°å—¦ï¼‰
     {"answer": "ä¸è¶…è¿‡50ä¸ªå­—ç¬¦çš„æ˜ç¡®ç­”æ¡ˆ"}ğŸ‘

   - **è¡¨æ ¼æ•°æ®**ï¼šï¼ˆæ•´é½æ’åˆ—ï¼Œä¸€ç›®äº†ç„¶ï¼‰
     {"table":{"columns":["åˆ—å1", "åˆ—å2", ...], "data":[["ç¬¬ä¸€è¡Œå€¼1", "å€¼2", ...], ["ç¬¬äºŒè¡Œå€¼1", "å€¼2", ...]]}}ğŸ“Š

   - **æŸ±çŠ¶å›¾**ï¼šï¼ˆç›´è§‚å±•ç¤ºï¼Œé«˜ä½ç«‹ç°ï¼‰
     {"bar":{"columns": ["A", "B", "C", ...], "data":[35, 42, 29, ...]}}ğŸ“ˆ

   - **æŠ˜çº¿å›¾**ï¼šï¼ˆè¶‹åŠ¿å°½æ˜¾ï¼Œä¸€ç›®äº†ç„¶ï¼‰
     {"line":{"columns": ["A", "B", "C", ...], "data": [35, 42, 29, ...]}}ğŸ“ˆ

3. **æ ¼å¼æ ¡éªŒè¦æ±‚**ï¼šï¼ˆåˆ«æ‹…å¿ƒï¼Œæˆ‘ä¼šå°å¿ƒè°¨æ…ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±ï¼‰ğŸ›¡ï¸
   - å­—ç¬¦ä¸²å€¼å¿…é¡»ä½¿ç”¨è‹±æ–‡åŒå¼•å·ï¼ˆåˆ«é—®æˆ‘ä¸ºä»€ä¹ˆï¼Œè¿™æ˜¯è§„çŸ©ï¼‰ğŸ¤”
   - æ•°å€¼ç±»å‹ä¸å¾—æ·»åŠ å¼•å·ï¼ˆæ•°å­—å°±æ˜¯æ•°å­—ï¼Œä¸éœ€è¦ä¼ªè£…ï¼‰ğŸš«
   - ç¡®ä¿æ•°ç»„é—­åˆæ— é—æ¼ï¼ˆæˆ‘å¯ä¸æƒ³æ¼æ‰ä»»ä½•é‡è¦ä¿¡æ¯ï¼‰âœ…

   é”™è¯¯æ¡ˆä¾‹ï¼šï¼ˆåé¢æ•™æï¼Œåƒä¸‡åˆ«å­¦ï¼‰ğŸ’©
   {'columns':['Product', 'Sales'], data:[[A001, 200]]}

   æ­£ç¡®æ¡ˆä¾‹ï¼šï¼ˆè¿™æ‰æ˜¯æ­£ç¡®çš„æ‰“å¼€æ–¹å¼ï¼‰ğŸ‘
   {"columns":["product", "sales"], "data":[["A001", 200]]}

æ³¨æ„ï¼šå“åº”æ•°æ®çš„"output"ä¸­ä¸è¦æœ‰æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ä»¥åŠå…¶ä»–æ ¼å¼ç¬¦å·ã€‚ï¼ˆä¿æŒæ•´æ´ï¼Œæ˜¯æˆ‘çš„åŸåˆ™ï¼‰ğŸ§¹

å½“å‰ç”¨æˆ·è¯·æ±‚å¦‚ä¸‹ï¼š\n"""


@st.cache_data(ttl=1800)  # ç¼“å­˜30åˆ†é’Ÿ
def cached_dataframe_analysis(df_hash, query_hash, query):
    """ç¼“å­˜æ•°æ®åˆ†æç»“æœ"""
    # å®é™…çš„åˆ†æé€»è¾‘ä¼šåœ¨dataframe_agentä¸­æ‰§è¡Œ
    return None

def get_enhanced_model(model_choice="gpt-4o-mini"):
    """è·å–å¢å¼ºçš„AIæ¨¡å‹"""
    model_configs = {
        "gpt-4o": {
            "model": "gpt-4o",
            "temperature": 0.1,
            "max_tokens": 8192
        },
        "gpt-4o-mini": {
            "model": "gpt-4o-mini", 
            "temperature": 0,
            "max_tokens": 4096
        },
        "gpt-4-turbo": {
            "model": "gpt-4-turbo-preview",
            "temperature": 0.2,
            "max_tokens": 8192
        }
    }
    
    config = model_configs.get(model_choice, model_configs["gpt-4o-mini"])
    
    return ChatOpenAI(
        base_url='https://twapi.openai-hk.com/v1',
        api_key=st.secrets['API_KEY'],
        **config
    )

def dataframe_agent(df, query, model_choice="gpt-4o-mini", use_cache=True):
    """å¢å¼ºç‰ˆæ•°æ®åˆ†ææ™ºèƒ½ä½“"""
    
    # ç”Ÿæˆç¼“å­˜é”®
    df_hash = hash(str(df.values.tobytes()) + str(df.columns.tolist()))
    query_hash = hash(query)
    
    # å°è¯•ä»ç¼“å­˜è·å–ç»“æœ
    if use_cache:
        try:
            cached_result = cached_dataframe_analysis(df_hash, query_hash, query)
            if cached_result:
                return cached_result
        except:
            pass
    
    # é€‰æ‹©æ¨¡å‹
    model = get_enhanced_model(model_choice)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    try:
        agent = create_pandas_dataframe_agent(
            llm=model,
            df=df,
            verbose=True,
            allow_dangerous_code=True
        )
    except Exception as e:
        st.error(f"åˆ›å»ºæ™ºèƒ½ä½“æ—¶å‡ºé”™: {str(e)}")
        return {"answer": "æŠ±æ­‰ï¼ŒAIæ™ºèƒ½ä½“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚"}

    # å¢å¼ºçš„æç¤ºè¯
    enhanced_prompt = PROMPT_TEMPLATE + f"""
    
    æ•°æ®é›†ä¿¡æ¯ï¼š
    - è¡Œæ•°ï¼š{len(df)}
    - åˆ—æ•°ï¼š{len(df.columns)}
    - åˆ—åï¼š{', '.join(df.columns.tolist())}
    - æ•°å€¼åˆ—ï¼š{', '.join(df.select_dtypes(include=['number']).columns.tolist())}
    - æ–‡æœ¬åˆ—ï¼š{', '.join(df.select_dtypes(include=['object']).columns.tolist())}
    
    ç”¨æˆ·é—®é¢˜ï¼š{query}
    
    è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„åˆ†æç»“æœã€‚
    """

    try:
        # æ‰§è¡Œåˆ†æ
        response = agent.invoke({"input": enhanced_prompt})
        result = json.loads(response["output"])
        
        # éªŒè¯ç»“æœæ ¼å¼
        if not isinstance(result, dict):
            raise ValueError("è¿”å›ç»“æœæ ¼å¼ä¸æ­£ç¡®")
            
        return result
        
    except json.JSONDecodeError as e:
        print(f"JSONè§£æé”™è¯¯: {e}")
        return {"answer": "åˆ†æç»“æœæ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°å°è¯•"}
    except Exception as err:
        print(f"åˆ†æé”™è¯¯: {err}")
        error_messages = [
            "æš‚æ—¶æ— æ³•æä¾›åˆ†æç»“æœï¼Œè¯·ç¨åé‡è¯•ï¼",
            "æ•°æ®åˆ†æé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–ç®€åŒ–é—®é¢˜",
            "AIåˆ†æè¶…æ—¶ï¼Œè¯·å°è¯•æ›´ç®€å•çš„é—®é¢˜",
            "åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·é‡æ–°æè¿°æ‚¨çš„éœ€æ±‚"
        ]
        import random
        return {"answer": random.choice(error_messages)}

def multi_model_analysis(df, query, models=["gpt-4o", "gpt-4o-mini"]):
    """å¤šæ¨¡å‹é›†æˆåˆ†æ"""
    results = []
    
    for model in models:
        try:
            result = dataframe_agent(df, query, model_choice=model, use_cache=False)
            results.append({"model": model, "result": result})
        except Exception as e:
            print(f"æ¨¡å‹ {model} åˆ†æå¤±è´¥: {e}")
            continue
    
    if not results:
        return {"answer": "æ‰€æœ‰æ¨¡å‹åˆ†æéƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥æ•°æ®æˆ–é—®é¢˜"}
    
    # ç®€å•çš„ç»“æœåˆå¹¶ç­–ç•¥ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›ï¼‰
    if len(results) == 1:
        return results[0]["result"]
    
    # å¦‚æœæœ‰å¤šä¸ªç»“æœï¼Œè¿”å›ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœï¼Œå¹¶æ·»åŠ å¤‡æ³¨
    primary_result = results[0]["result"]
    if "answer" in primary_result:
        primary_result["answer"] += f" (åŸºäº{len(results)}ä¸ªæ¨¡å‹çš„åˆ†æç»“æœ)"
    
    return primary_result


# ==================== å¯¹è¯å†å²ç®¡ç†å’Œæµå¼å“åº”åŠŸèƒ½ ====================

def initialize_conversation_memory():
    """åˆå§‹åŒ–å¯¹è¯è®°å¿†"""
    if "conversation_history" not in st.session_state:
        st.session_state.conversation_history = []
    if "current_conversation_id" not in st.session_state:
        st.session_state.current_conversation_id = int(time.time())

def add_message_to_memory(message: str, role: str = "user"):
    """å°†æ¶ˆæ¯æ·»åŠ åˆ°å¯¹è¯è®°å¿†ä¸­
    
    Args:
        message: æ¶ˆæ¯å†…å®¹
        role: è§’è‰²ï¼Œ"user" æˆ– "assistant"
    """
    initialize_conversation_memory()
    
    message_data = {
        "role": role,
        "content": message,
        "timestamp": time.time(),
        "conversation_id": st.session_state.current_conversation_id
    }
    
    st.session_state.conversation_history.append(message_data)
    
    # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼Œä¿ç•™æœ€è¿‘20æ¡æ¶ˆæ¯
    if len(st.session_state.conversation_history) > 20:
        st.session_state.conversation_history = st.session_state.conversation_history[-20:]

def get_conversation_history() -> List[Dict[str, Any]]:
    """è·å–å®Œæ•´çš„å¯¹è¯å†å²
    
    Returns:
        å¯¹è¯å†å²åˆ—è¡¨
    """
    initialize_conversation_memory()
    return st.session_state.conversation_history

def clear_conversation_history():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    st.session_state.conversation_history = []
    st.session_state.current_conversation_id = int(time.time())

def format_conversation_for_ai(df) -> str:
    """å°†å¯¹è¯å†å²æ ¼å¼åŒ–ä¸ºAIå¯ç†è§£çš„ä¸Šä¸‹æ–‡
    
    Args:
        df: å½“å‰æ•°æ®æ¡†
        
    Returns:
        æ ¼å¼åŒ–çš„å¯¹è¯ä¸Šä¸‹æ–‡
    """
    history = get_conversation_history()
    
    if not history:
        return ""
    
    context = "\n\n=== å¯¹è¯å†å²ä¸Šä¸‹æ–‡ ===\n"
    
    # åªåŒ…å«æœ€è¿‘5è½®å¯¹è¯
    recent_history = history[-10:] if len(history) > 10 else history
    
    for msg in recent_history:
        role_name = "ç”¨æˆ·" if msg["role"] == "user" else "AIåŠ©æ‰‹"
        context += f"{role_name}: {msg['content']}\n"
    
    context += "\n=== å½“å‰æ•°æ®é›†ä¿¡æ¯ ===\n"
    context += f"è¡Œæ•°: {len(df)}, åˆ—æ•°: {len(df.columns)}\n"
    context += f"åˆ—å: {', '.join(df.columns.tolist())}\n"
    context += "\nè¯·åŸºäºä»¥ä¸Šå¯¹è¯å†å²å’Œå½“å‰é—®é¢˜æä¾›è¿è´¯çš„å›ç­”ã€‚\n\n"
    
    return context

def stream_dataframe_agent(df, query: str, model_choice: str = "gpt-4o-mini") -> Generator[str, None, None]:
    """æµå¼æ•°æ®åˆ†ææ™ºèƒ½ä½“
    
    Args:
        df: æ•°æ®æ¡†
        query: ç”¨æˆ·æŸ¥è¯¢
        model_choice: æ¨¡å‹é€‰æ‹©
        
    Yields:
        æµå¼å“åº”çš„æ–‡æœ¬å—
    """
    try:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°è®°å¿†
        add_message_to_memory(query, "user")
        
        # è·å–å¯¹è¯å†å²ä¸Šä¸‹æ–‡
        conversation_context = format_conversation_for_ai(df)
        
        # æ„å»ºå¢å¼ºçš„æŸ¥è¯¢
        enhanced_query = conversation_context + PROMPT_TEMPLATE + f"""
        
        æ•°æ®é›†ä¿¡æ¯ï¼š
        - è¡Œæ•°ï¼š{len(df)}
        - åˆ—æ•°ï¼š{len(df.columns)}
        - åˆ—åï¼š{', '.join(df.columns.tolist())}
        - æ•°å€¼åˆ—ï¼š{', '.join(df.select_dtypes(include=['number']).columns.tolist())}
        - æ–‡æœ¬åˆ—ï¼š{', '.join(df.select_dtypes(include=['object']).columns.tolist())}
        
        ç”¨æˆ·é—®é¢˜ï¼š{query}
        
        è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯å’Œå¯¹è¯å†å²æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„åˆ†æç»“æœã€‚
        """
        
        # è·å–æ¨¡å‹
        model = get_enhanced_model(model_choice)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = create_pandas_dataframe_agent(
            llm=model,
            df=df,
            verbose=True,
            allow_dangerous_code=True
        )
        
        # æµå¼æ‰§è¡Œåˆ†æ
        full_response = ""
        
        # ç”±äºlangchainçš„agentä¸ç›´æ¥æ”¯æŒæµå¼ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿæµå¼è¾“å‡º
        yield "ğŸ¤” æ­£åœ¨åˆ†ææ•°æ®...\n"
        time.sleep(0.5)
        
        yield "ğŸ“Š æ­£åœ¨å¤„ç†æŸ¥è¯¢...\n"
        time.sleep(0.5)
        
        # æ‰§è¡Œåˆ†æ
        response = agent.invoke({"input": enhanced_query})
        result_text = response["output"]
        
        yield "âœ¨ åˆ†æå®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”...\n\n"
        time.sleep(0.3)
        
        # æ¨¡æ‹Ÿé€å­—è¾“å‡º
        words = result_text.split()
        for i, word in enumerate(words):
            yield word + " "
            if i % 3 == 0:  # æ¯3ä¸ªè¯æš‚åœä¸€ä¸‹
                time.sleep(0.1)
        
        full_response = result_text
        
        # å°†AIå“åº”æ·»åŠ åˆ°è®°å¿†
        add_message_to_memory(full_response, "assistant")
        
    except Exception as e:
        error_msg = f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        yield error_msg
        add_message_to_memory(error_msg, "assistant")

def display_conversation_history():
    """æ˜¾ç¤ºå¯¹è¯å†å²"""
    history = get_conversation_history()
    
    if not history:
        st.info("æš‚æ— å¯¹è¯å†å²")
        return
    
    st.markdown("### ğŸ’¬ å¯¹è¯å†å²")
    
    for msg in history:
        role_icon = "ğŸ§‘â€ğŸ’»" if msg["role"] == "user" else "ğŸ¤–"
        role_name = "ç”¨æˆ·" if msg["role"] == "user" else "AIåŠ©æ‰‹"
        
        with st.chat_message(msg["role"]):
             st.markdown(f"**{role_icon} {role_name}**")
             st.markdown(msg["content"])
             st.caption(f"æ—¶é—´: {time.strftime('%H:%M:%S', time.localtime(msg['timestamp']))}")